*****************
**** July 12, 2017

*****************
**** July 10-11, 2017
WDH: Look at Qi's issue when the sphere approaches the bottom

PARALLEL -- WRONG
****** TEMP update all mappings before ogen call t3=1.125e+00***TEMP**
grid=1 boundingBox=[-1.166e+00,1.166e+00][8.188e-01,3.152e+00][-5.129e-01,1.167e+00]


 --MVG-- use resetToFirstPriority (full update) in Ogen::updateOverlap t3=1.225e+00+++++ 
 ****** TEMP update all mappings before ogen call t3=1.225e+00***TEMP**
grid=1 boundingBox=[-1.166e+00,1.166e+00][8.303e-01,3.163e+00][-5.129e-01,1.167e+00]


SERIAL -- BOUNDING BOX IS OK: 

--MVG:RB-- body 0 : t2=1.2000e+00 fCM=(-1.95e-05,7.96e+00,2.68e-05) torque=(-4.87e-06,2.01e-08,1.04e-07)
--RBM-- getAcceleration:  t=1.20e+00, mass=5.81e-01, a-provided = (-3.35e-05,4.63e-02,4.06e-05), ip=2, ipp1=3, beta=0.0e+00 times=[1.175e+00,1.200e+00] a1=[1.107e-05,-3.348e-05] numberProvided=5 

 --MVG-- use resetToFirstPriority (full update) in Ogen::updateOverlap +++++++++++++ 
 ****** TEMP update all mappings before ogen call *****  *TEMP**
grid=1 boundingBox=[-1.166e+00,1.166e+00][8.165e-02,2.415e+00][-5.131e-01,1.167e+00]





Qi:
Try these two grids. The first one has checkOG errors (after t=1.2) but the second one has no such errors.
ogen -noplot drop3d -factor=1 -interp=e -xa=-1.5 -xb=1.5 -za=-1.5 -zb=1.5 -yb=3.5 -yr=2 -name=shorterDropDebug1
ogen -noplot drop3d -factor=1 -interp=e -xa=-1.5 -xb=1.5 -za=-1.5 -zb=1.5 -yb=3.5 -yr=1.5 -name=shorterDropDebug2

RUN AGAIN:
mpirun -np 1 $cginsp -noplot sphereDrop -g=shorterDropDebug1e1.order2.hdf -tf=1.3 -tp=.05 -ts=im -radius=.5 -dropName=drop -channelName=channel -dtMax=.025 -project=0 -nu=0.67462470 -numberOfCorrections=2 -addedMass=1 -useProvidedAceration=1 -addedDamping=1 -addedDampingCoeff=1 -addedDampingProjectVelocity=1 -scaleAddedDampingWithDt=1 -useTP=0 -debug=3 -solver=best -psolver=best -rtol=1.e-7 -atol=1.e-9 -rtolp=1.e-5 -atolp=1.e-7 -freqFullUpdate=1 -bcOption=outflow  -ogesDebug=1 -show=sphereDropA.show -go=go >! sphereDropDebug1A.out


mpirun -np 1 $cginsp -noplot sphereDrop -g=shorterDropDebug1e1.order2.hdf -tf=1.3 -tp=.05 -ts=im -radius=.5 -dropName=drop -channelName=channel -dtMax=.025 -project=0 -nu=0.67462470 -numberOfCorrections=2 -addedMass=1 -useProvidedAceration=1 -addedDamping=1 -addedDampingCoeff=1 -addedDampingProjectVelocity=1 -scaleAddedDampingWithDt=1 -useTP=0 -debug=3 -solver=best -psolver=best -rtol=1.e-7 -atol=1.e-9 -rtolp=1.e-5 -atolp=1.e-7 -freqFullUpdate=1 -bcOption=outflow  -ogesDebug=1 -show=sphereDrop.show -go=go >! sphereDropDebug1.out


>>>>>>>>>>>>>>> GRID 
ogen -noplot drop3d -factor=1 -interp=e -xa=-1.5 -xb=1.5 -za=-1.5 -zb=1.5 -yb=2.75 -yr=1.3 -name=shorterDrop3d

mpirun -np 1 $cginsp -noplot sphereDrop -g=shorterDrop3de1.order2.hdf -tf=.4 -tp=.05 -ts=im -radius=.5 -dropName=drop -channelName=channel -dtMax=.025 -project=0 -nu=0.67462470 -numberOfCorrections=2 -addedMass=1 -useProvidedAceration=1 -addedDamping=1 -addedDampingCoeff=1 -addedDampingProjectVelocity=1 -scaleAddedDampingWithDt=1 -useTP=0 -debug=3 -solver=best -psolver=best -rtol=1.e-7 -atol=1.e-9 -rtolp=1.e-5 -atolp=1.e-7 -freqFullUpdate=1 -bcOption=outflow  -ogesDebug=1 -show=sphereDrop.show -go=go >! sphereDrop.out




Make an even smaller domain
IMPLICIT INTERPOLATION
ogen -noplot drop3d -factor=1 -interp=i -xa=-1.5 -xb=1.5 -za=-1.5 -zb=1.5 -yb=2.5 -yr=1. -name=shorterDrop3d

mpirun -np 1 $cginsp sphereDrop -g=shorterDrop3de1.order2.hdf -tf=2.25 -tp=.01 -ts=im -radius=.5 -dropName=drop -channelName=channel -dtMax=.025 -project=0 -nu=0.67462470 -numberOfCorrections=2 -addedMass=1 -useProvidedAceration=1 -addedDamping=1 -addedDampingCoeff=1 -addedDampingProjectVelocity=1 -scaleAddedDampingWithDt=1 -useTP=0 -debug=3 -solver=best -psolver=best -rtol=1.e-7 -atol=1.e-9 -rtolp=1.e-5 -atolp=1.e-7 -freqFullUpdate=1 -bcOption=outflow -go=halt -ogesDebug=1

mpirun -np 1 $cginsp sphereDrop -g=shorterDrop3di1.order2.hdf -tf=2.25 -tp=.01 -ts=im -radius=.5 -dropName=drop -channelName=channel -dtMax=.025 -project=0 -nu=0.67462470 -numberOfCorrections=2 -addedMass=1 -useProvidedAceration=1 -addedDamping=1 -addedDampingCoeff=1 -addedDampingProjectVelocity=1 -scaleAddedDampingWithDt=1 -useTP=0 -debug=3 -solver=best -psolver=best -rtol=1.e-7 -atol=1.e-9 -rtolp=1.e-5 -atolp=1.e-7 -freqFullUpdate=1 -bcOption=outflow -go=halt -ogesDebug=1



BACKUP INTERPOLATION:
ogen -noplot drop3d -factor=1 -interp=e -xa=-1.5 -xb=1.5 -za=-1.5 -zb=1.5 -yb=2.5 -yr=1. -name=shorterDrop3d

ogen -noplot drop3d -factor=1 -interp=e -xa=-1.5 -xb=1.5 -za=-1.5 -zb=1.5 -yb=2.5 -yr=1. -name=shorterDrop3d

mpirun -np 1 ogenx -noplot drop3d -factor=1 -interp=e -xa=-1.5 -xb=1.5 -za=-1.5 -zb=1.5 -yb=2.5 -yr=1. -name=shorterDrop3d

mpirun -np 1 $cginsp sphereDrop -g=shorterDrop3de1.order2.hdf -tf=2.25 -tp=.01 -ts=im -radius=.5 -dropName=drop -channelName=channel -dtMax=.025 -project=0 -nu=0.67462470 -numberOfCorrections=2 -addedMass=1 -useProvidedAceration=1 -addedDamping=1 -addedDampingCoeff=1 -addedDampingProjectVelocity=1 -scaleAddedDampingWithDt=1 -useTP=0 -debug=3 -solver=best -psolver=best -rtol=1.e-7 -atol=1.e-9 -rtolp=1.e-5 -atolp=1.e-7 -freqFullUpdate=1 -bcOption=outflow -go=halt -ogesDebug=1

lastChanceInterpolation:ERROR: unable to get proper interpolation for pt on grid=1 (transform[grid=1]), (i1,i2,i3)=(3,-1,3) 


mpirun -np 1 $cginsp sphereDrop -g=shortDrop3de1.order2.hdf -tf=2.25 -tp=.01 -ts=im -radius=.5 -dropName=drop -channelName=channel -dtMax=.025 -project=0 -nu=0.67462470 -numberOfCorrections=2 -addedMass=1 -useProvidedAceration=1 -addedDamping=1 -addedDampingCoeff=1 -addedDampingProjectVelocity=1 -scaleAddedDampingWithDt=1 -useTP=0 -debug=3 -solver=best -psolver=best -rtol=1.e-7 -atol=1.e-9 -rtolp=1.e-5 -atolp=1.e-7 -freqFullUpdate=1 -bcOption=outflow -go=halt -ogesDebug=1


******************* Qi 
this case is based on drop3d.cmd in SampleGrid

++++++++GRIDS++++++++
ogen -noplot drop3d -factor=1 -interp=e

smaller domain for quick test
ogen -noplot drop3d -factor=1 -interp=e -yb=5 -yr=3 -name=shortDrop3d

-------QUICK TEST------
quick test on a smaller domain (tfinal=2.25 and it only takes 2 mins to run)
$cgins -noplot sphereDrop -g=shortDrop3de1.order2.hdf -tf=2.25 -tp=.5 -ts=im -radius=.5 -dropName=drop -channelName=channel -dtMax=.025 -project=0 -nu=0.67462470 -numberOfCorrections=2 -addedMass=1 -useProvidedAceration=1 -addedDamping=1 -addedDampingCoeff=1 -addedDampingProjectVelocity=1 -scaleAddedDampingWithDt=1 -useTP=0 -debug=3 -solver=best -psolver=best -rtol=1.e-7 -atol=1.e-9 -rtolp=1.e-5 -atolp=1.e-7 -freqFullUpdate=1 -bcOption=outflow -ogesDebug=1 -go=go > junk.out &

compare to 
mpirun -np 1 $cginsp noplot sphereDrop -g=shortDrop3de1.order2.hdf -tf=2.25 -tp=.5 -ts=im -radius=.5 -dropName=drop -channelName=channel -dtMax=.025 -project=0 -nu=0.67462470 -numberOfCorrections=2 -addedMass=1 -useProvidedAceration=1 -addedDamping=1 -addedDampingCoeff=1 -addedDampingProjectVelocity=1 -scaleAddedDampingWithDt=1 -useTP=0 -debug=3 -solver=best -psolver=best -rtol=1.e-7 -atol=1.e-9 -rtolp=1.e-5 -atolp=1.e-7 -freqFullUpdate=1 -bcOption=outflow -go=go -ogesDebug=1 > junknp1.out &

issues in parallel code
issue 1: build matrix is slow for pressure
* np=1 build pressure matrix =5seconds while build velocity matrix only needs 0.1 second
* serial only needs 1e-3 second to build 
* it becomes more obvious when the grid becomes finer
issue 2: there are checkOG:ERROR if spherical grids are out of the rectangular domain (may be okay)
* the first error happens at t=1.925 for np=1 or 4 when two ghost points are out of domian
* the issue is independent of np, which seems a good news


-------FULL TEST------
test in rbins3d.pdf
G1 (needs 18 mins to run)
$cgins -noplot sphereDrop -g=drop3de1.order2.hdf -tf=7.3 -tp=.5 -ts=im -radius=.5 -dropName=drop -channelName=channel -dtMax=.025 -project=0 -nu=0.67462470 -numberOfCorrections=2 -addedMass=1 -useProvidedAceration=1 -addedDamping=1 -addedDampingCoeff=1 -addedDampingProjectVelocity=1 -scaleAddedDampingWithDt=1 -useTP=0 -debug=3 -solver=best -psolver=best -rtol=1.e-7 -atol=1.e-9 -rtolp=1.e-5 -atolp=1.e-7 -freqFullUpdate=1 -bcOption=outflow -go=go -show=particleSettleG1.show > particleSettleG1.out &




6/19
add a short test

6/15/2017
serial test (G1 can run up to tfinal=7.3):
$cgins -noplot sphereDrop -g=drop3de1.order2.hdf -tf=7.3 -tp=.5 -ts=im -radius=.5 -dropName=drop -channelName=channel -dtMax=.025 -project=0 -nu=0.67462470 -numberOfCorrections=2 -addedMass=1 -useProvidedAceration=1 -addedDamping=1 -addedDampingCoeff=1 -addedDampingProjectVelocity=1 -scaleAddedDampingWithDt=1 -useTP=0 -debug=3 -solver=best -psolver=best -rtol=1.e-7 -atol=1.e-9 -rtolp=1.e-5 -atolp=1.e-7 -freqFullUpdate=1 -bcOption=outflow -go=go -show=particleSettleG1.show > particleSettleG1.out &

parallel test 
mpirun -np 8 $cginsp -noplot sphereDrop -g=drop3de1.order2.hdf -tf=7.3 -tp=.5 -ts=im -radius=.5 -dropName=drop -channelName=channel -dtMax=.025 -project=0 -nu=0.67462470 -numberOfCorrections=2 -addedMass=1 -useProvidedAceration=1 -addedDamping=1 -addedDampingCoeff=1 -addedDampingProjectVelocity=1 -scaleAddedDampingWithDt=1 -useTP=0 -debug=3 -solver=best -psolver=best -rtol=1.e-7 -atol=1.e-9 -rtolp=1.e-5 -atolp=1.e-7 -freqFullUpdate=1 -bcOption=outflow -go=go -show=particleSettleG1np8.show > particleSettleG1np8.out &

Issue 1: PETScSolver.C takes 10 seconds to build any pressure matrix if np=1, which takes more than half of the total computational time! np>1 it gets significantly reduced, 0.5s if np=4 on dev-intel16
PETSc build matrix test (need turned off some output in PETScSolver.bC and generateMatrix.C, otherwise it is going to be too much):
Compare np=1 vs np=4
mpirun -np 4 $cginsp -noplot sphereDrop -g=drop3de1.order2.hdf -tf=1 -tp=.5 -ts=im -radius=.5 -dropName=drop -channelName=channel -dtMax=.025 -project=0 -nu=0.67462470 -numberOfCorrections=2 -addedMass=1 -useProvidedAceration=1 -addedDamping=1 -addedDampingCoeff=1 -addedDampingProjectVelocity=1 -scaleAddedDampingWithDt=1 -useTP=0 -debug=3 -solver=best -psolver=best -rtol=1.e-7 -atol=1.e-9 -rtolp=1.e-5 -atolp=1.e-7 -freqFullUpdate=1 -flushFrequency=10 -bcOption=outflow -go=go -ogesDebug=1

serial test: build matrix is extremely fast
$cgins -noplot sphereDrop -g=drop3de1.order2.hdf -tf=1 -tp=.5 -ts=im -radius=.5 -dropName=drop -channelName=channel -dtMax=.025 -project=0 -nu=0.67462470 -numberOfCorrections=2 -addedMass=1 -useProvidedAceration=1 -addedDamping=1 -addedDampingCoeff=1 -addedDampingProjectVelocity=1 -scaleAddedDampingWithDt=1 -useTP=0 -debug=3 -solver=best -psolver=best -rtol=1.e-7 -atol=1.e-9 -rtolp=1.e-5 -atolp=1.e-7 -freqFullUpdate=1 -flushFrequency=10 -bcOption=outflow -go=go -ogesDebug=3

Issue 2: checkOG:ERROR occurs as long as the grid point is outside of domain; not seen in serial code
see the parallel test; happens around t=7

########### the following is my notes; ignore ################
drop3d on hpcc=dropFine on cge; they both produced by drop3d

explicit G1 can only run to tf=7.3 (implicit can run to tf=7.5)

G1 (drop3de1)
serial; run successfully to tf=7.3 without any error
cpu=1.09e+03 memory=672.93 Mb, 405.9 reals/(grid-pt)
$cgins -noplot sphereDrop -g=drop3de1.order2.hdf -tf=7.3 -tp=.5 -ts=im -radius=.5 -dropName=drop -channelName=channel -dtMax=.025 -project=0 -nu=0.67462470 -numberOfCorrections=2 -addedMass=1 -useProvidedAceration=1 -addedDamping=1 -addedDampingCoeff=1 -addedDampingProjectVelocity=1 -scaleAddedDampingWithDt=1 -useTP=0 -debug=3 -solver=best -psolver=best -rtol=1.e-7 -atol=1.e-9 -rtolp=1.e-5 -atolp=1.e-7 -freqFullUpdate=1 -flushFrequency=10 -bcOption=outflow -go=go -show=particleSettleG1.show > particleSettleG1.out &

parallel
conclusion: np=1 np=8 and serial results are very closed but checkOG:ERROR will appear as soon as the grid is out of bounds (verify it in 2D!!)
its effects are not clear; certainly it fails to write the last show file; but I am not sure how much it will affect the solution

np=1 particleSettleG1np1.out; very slow
estimated cpu=4hours; estimated memory=1GB
cpu= memory

np=8 -show=particleSettleG1np8.show; 
issue: checkOG:ERROR with negative interp weights
cpu= memory=

drop3di1

np=8 a bug seems appear but cannot reproduce. very strange

test np=1

$cgins -noplot sphereDrop -g=drop3de1.order2.hdf -tf=1 -tp=.5 -ts=im -radius=.5 -dropName=drop -channelName=channel -dtMax=.025 -project=0 -nu=0.67462470 -numberOfCorrections=2 -addedMass=1 -useProvidedAceration=1 -addedDamping=1 -addedDampingCoeff=1 -addedDampingProjectVelocity=1 -scaleAddedDampingWithDt=1 -useTP=0 -debug=3 -solver=best -psolver=best -rtol=1.e-7 -atol=1.e-9 -rtolp=1.e-5 -atolp=1.e-7 -freqFullUpdate=1 -flushFrequency=10 -bcOption=outflow -go=go > testG1.out &

mpirun -np 1 $cginsp -noplot sphereDrop -g=drop3de1.order2.hdf -tf=1 -tp=.5 -ts=im -radius=.5 -dropName=drop -channelName=channel -dtMax=.025 -project=0 -nu=0.67462470 -numberOfCorrections=2 -addedMass=1 -useProvidedAceration=1 -addedDamping=1 -addedDampingCoeff=1 -addedDampingProjectVelocity=1 -scaleAddedDampingWithDt=1 -useTP=0 -debug=3 -solver=best -psolver=best -rtol=1.e-7 -atol=1.e-9 -rtolp=1.e-5 -atolp=1.e-7 -freqFullUpdate=1 -flushFrequency=10 -bcOption=outflow -go=go > testG1np1.out &

long time to build matrix
1044 PETScSolver:: ... done build matrix, cpu=1.16e+01 
1185 PETScSolver:: ... done build matrix, cpu=1.18e+01
1488 PETScSolver:: ... done build matrix, cpu=1.24e+01
1608 PETScSolver:: ... done build matrix, cpu=1.22e+01


G2
$cgins -noplot sphereDrop -g=drop3de2.order2.hdf -tf=8.5 -tp=.5 -ts=im -radius=.5 -dropName=drop -channelName=channel -dtMax=.025 -project=0 -nu=0.67462470 -numberOfCorrections=2 -addedMass=1 -useProvidedAceration=1 -addedDamping=1 -addedDampingCoeff=1 -addedDampingProjectVelocity=1 -scaleAddedDampingWithDt=1 -useTP=0 -debug=3 -solver=best -psolver=best -rtol=1.e-7 -atol=1.e-9 -rtolp=1.e-5 -atolp=1.e-7 -freqFullUpdate=1 -flushFrequency=10 -bcOption=outflow -go=go -show=particleSettleG2.show > particleSettleG2.out &


