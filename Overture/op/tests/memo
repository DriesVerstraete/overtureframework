***************************************************************************
****** June 3,4 2017

*** CHANGED PETSC-SOLVER TO USE OGES EQUATION NUMBERING TOO


*NEW*
mpirun -np 1 $tcmcp square8.order2.hdf -solver=petsc -predefined > tc.np1.out
mpirun -np 2 $tcmcp square8.order2.hdf -solver=petsc -predefined > tc.np2.out
mpirun -np 3 $tcmcp square8.order2.hdf -solver=petsc -predefined > tc.np3.out
mpirun -np 4 $tcmcp square8.order2.hdf -solver=petsc -predefined > tc.np4.out


OLD:
mpirun -np 1 $tcmcp square8.order2.hdf -solver=petsc -predefined > tcmc.np1.out
mpirun -np 2 $tcmcp square8.order2.hdf -solver=petsc -predefined > tcmc.np2.out
mpirun -np 3 $tcmcp square8.order2.hdf -solver=petsc -predefined > tcmc.np3.out
mpirun -np 4 $tcmcp square8.order2.hdf -solver=petsc -predefined > tcmc.np4.out


*** CHANGE PETScSOLVER to put extra equations at the end of the matrix 


mpirun -np 1 $tcmcp square8.order2.hdf -solver=petsc -predefined > tc.np1.out




set tcmcp = $op/tests/tcmConstraintp 
mpirun -np 1 $tcmcp sliderRefineGride2.order2 -solver=petsc -predefined -trig

Maximum relative error with dirichlet bc's= 8.996811e-03 (8.996811e-03 with ghost)
--TCMC-- i=0 extra=1 extraEquationNumber(0) : constraint value= 1.958 true= 2.000 err=4.24e-02
--TCMC-- i=1 extra=0 extraEquationNumber(1) : constraint value= 0.938 true= 1.000 err=6.25e-02

Maximum relative error with neumann bc's= 8.375221e-03
--TCMC-- i=0 extra=1 extraEquationNumber(0) : constraint value=11.005 true=11.000 err=4.51e-03
--TCMC-- i=1 extra=0 extraEquationNumber(1) : constraint value=-0.000 true= 0.000 err=6.54e-13



***************************************************************************
****** June 2, 2017


TEST TCMCONSTRAINT ON overset grid


**** SLIDER GRID IS PERIODIC IN X ***

set tcmcp = $op/tests/tcmConstraintp 
mpirun -np 1 $tcmcp sliderRefineGride2.order2 -solver=petsc -predefined 

SERIAL: NOT EXACT **WHY**
$tcmcs sliderRefineGride2.order2 -solver=petsc -predefined  > sr.serial.out

WORKS NOW: 
mpirun -np 1 $tcmcp plug2.hdf -solver=petsc -predefined > plug.np1.out
mpirun -np 1 $tcmcp plug1.hdf -solver=petsc -predefined > plug1.np1.out
SERIAL PLUG -- exact 
$tcmcs plug2.hdf -solver=petsc -predefined  > plug.serial.out
$tcmcs plug1.hdf -solver=petsc -predefined  > plug1.serial.out

mpirun -np 1 $tcmcp square8.order2.hdf -solver=petsc -predefined > tcmc.np1.out



***************************************************************************
****** June 1, 2017

**CHANGES TO USER SUPPLIED EQUATIONS IN PARALLEL***

*NEW* TEST:

  
WORKS: 
mpirun -np 3 valgrind --log-file=tc.out.np%p $tcmcp square8.order2.hdf -solver=petsc -predefined
mpirun -np 2 valgrind --log-file=tc.out.np%p $tcmcp square8.order2.hdf -solver=petsc -predefined

mpirun -np 1 valgrind $tcmcp square8.order2.hdf -solver=petsc -predefined

-- looks OK, check valgrind
mpirun -np 1 $tcmcp square8.order2.hdf -solver=petsc -predefined > tcmc.np1.out
mpirun -np 2 $tcmcp square8.order2.hdf -solver=petsc -predefined > tcmc.np2.out
mpirun -np 3 $tcmcp square8.order2.hdf -solver=petsc -predefined > tcmc.np3.out
mpirun -np 4 $tcmcp square8.order2.hdf -solver=petsc -predefined > tcmc.np4.out



set tcmcp = $op/tests/tcmConstraintp 
set tcmcs = $op/tests/tcmConstraints

$tcmcp square8.order2.hdf -solver=petsc -predefined
$tcmcs square8.order2.hdf -solver=petsc -predefined




***************************************************************************
****** May 23, 2017

ANOTHER BUG -- Shift equationNumber base in PETScSolver to eqnBase=1

$tcmcp square8.order2.hdf -solver=petsc -predefined > ! tcmcParallelpNew.out

tcm3p -g=shearBlockCoarseGrid2.order2.hdf -solver=petsc -debug=15 > ! matParallelNew.out



***************************************************************************
****** May 21-22, 2017

** ANOTHER BUG FOUND with predefined and user-defined equations

**BUG FOUND WITH PARALLEL and PREDEFINED -- constraint equation bad


CHECK tcmConstraint + predefined:

set tcmcp = $op/tests/tcmConstraintp 
set tcmcs = $op/tests/tcmConstraints

$tcmcp square8.order2.hdf -solver=petsc -predefined > ! tcmcParallelp.out
$tcmcs square8.order2.hdf -solver=petsc -predefined > ! tcmcSerials.out



! These match now:
tcm3p -g=shearBlockCoarseGrid2.order2.hdf -solver=petsc -predefined > ! matParallelp.out
tcm3s -g=shearBlockCoarseGrid2.order2.hdf -solver=petsc -predefined > ! matSerialp.out

! These match now:
tcm3p -g=shearBlockCoarseGrid2.order2.hdf -solver=petsc -debug=7 > ! matParallel.out
tcm3s -g=shearBlockCoarseGrid2.order2.hdf -solver=petsc -debug=7 > ! matSerial.out

***************************************************************************
****** May 21, 2017


Tracking down bug with parallel AMP:

alias tcm3p $op/tests/tcm3p
alias tcm3s $op/tests/tcm3s

tcm3p -g=shearBlockCoarseGrid2.order2.hdf -solver=petsc > ! matParallel.out

tcm3 -g=shearBlockCoarseGrid2.order2.hdf -solver=petsc > ! matSerial.out


tcm3 -g=shearBlockCoarseGrid2.order2.hdf -solver=petsc -neumann -outputMatrix


***************************************************************************
****** April 1-2, 2017


MORE WORK ON CONSTRAINTS IN PARALLEL PETSC

PRINT OUT SPARSE MATRIX

mpiexec -n 2 xterm -e gdb --args $tcmc square5.order2.hdf -solver=petsc -tol=1.e-12

PARALLEL
mpirun -np 1 $tcmc square5.order2.hdf -solver=petsc -tol=1.e-12 >! tcmc.parallel.out


SERIAL
$tcmc square5.order2.hdf -solver=petsc -tol=1.e-12 >! tcmc.serial.out


***************************************************************************
****** March 13-20, 2017

Implement constraints in parallel petsc

mpiexec -n 2 xterm -e gdb --args $tcmc square8.order2.hdf -solver=petsc


set tcmc = $op/tests/tcmConstraint

$tcmc square8.order2.hdf -solver=petsc

mpirun -np 1 $tcmc square8.order2.hdf -solver=petsc



  


============================
==== Aug 27, 2016

----- TEST NEW opt version of 6th-order Laplacian ----

mpiexec -n 4 tcm3 -g=square64p.order6.hdf -trig -neumann -order=6 -predefined -solver=petsc -tol=1.e-12
 grid=0 (square) max. rel. err=1.599507e-09 (1.599616e-09 with ghost)

mpiexec -n 2 tcm3 -g=square32p.order6.hdf -trig -neumann -order=6 -predefined -solver=petsc
 grid=0 (square) max. rel. err=1.035096e-07 (1.035096e-07 with ghost)


tcm3 -g=square32p.order6.hdf -trig -neumann -order=6 -predefined

grid=0 (square) max. rel. err=1.017155e-07 (1.017155e-07 with ghost)


--- TEST predefined and 6th order --

mpiexec -n 2 xterm -e gdb --args tcm3 -g=square32p.order6.hdf -trig -neumann -order=6 -predefined -solver=petsc


mpirun -np 2 tcm3 -g=square32p.order6.hdf -trig -neumann -order=6 -predefined -solver=petsc



mpirun -np 1 tcm3 -g=square32p.order6.hdf -trig -neumann -order=6 -predefined -solver=petsc
 grid=0 (square) max. rel. err=1.123300e-07 (1.123300e-07 with ghost)

============================
==== Aug 18, 2016

--- TEST predefined and 6th order --


tcm3 -g=square32p.order6.hdf -trig -neumann -order=6 -predefined
 grid=0 (square) max. rel. err=1.017155e-07 (1.017155e-07 with ghost)

tcm3 -g=square64p.order6.hdf -trig -neumann -order=6 -predefined
 grid=0 (square) max. rel. err=1.596434e-09 (1.596434e-09 with ghost)

tcm3 -g=square128p.order6.hdf -trig -neumann -order=6 -predefined
 grid=0 (square) max. rel. err=2.472844e-11 (2.472844e-11 with ghost)




tcm3 -g=square64p.order6.hdf -trig -neumann -order=6
 grid=0 (square) max. rel. err=1.596434e-09 (1.596434e-09 with ghost)

============================
==== Aug 17, 2016

TEST sixth-order for Jeff

*FIXED* laplacianFDCoefficients>c (old style)

Looks OK -- ratio 64
tcm3 -g=square64p.order6.hdf -trig -neumann -order=6
tcm3 -g=square32p.order6.hdf -trig -neumann -order=6
G32: grid=0 (square) max. rel. err=1.017155e-07 (1.017155e-07 with ghost)
G64: grid=0 (square) max. rel. err=1.596434e-09 (1.596434e-09 with ghost)
G128:grid=0 (square) max. rel. err=2.472844e-11 (2.472844e-11 with ghost)


ORDER=6  -- but results are only 4th-order
tcm3 -g=square32p.order6.hdf -trig -neumann -order=6
G32: grid=0 (square) max. rel. err=1.645847e-05 (1.645847e-05 with ghost)
G64: grid=0 (square) max. rel. err=1.031297e-06 (1.031297e-06 with ghost)
G128:grid=0 (square) max. rel. err=6.449760e-08 (6.449760e-08 with ghost)


tcm3 -g=square8p.order6.hdf -trig -neumann -order=6



ORDER=4
tcm3 -g=square32p.order4.hdf -trig -neumann -order=4
grid=0 (square) max. rel. err=1.645847e-05 (1.645847e-05 with ghost)


-- periodic order 2
tcm3 -g=square32p.order2.hdf -trig -neumann
grid=0 (square) max. rel. err=3.218964e-03 (3.218964e-03 with ghost)


============================
==== July 29, 2016

Test new constraint

tcmConstraint square8.order2.hdf -solver=petsc


tcmConstraint square8.order2.hdf -neumann

****** Oct 11-12, 14 2015.

--- Neumann:

tcmConstraint square8.order2.hdf -neumann

tcmConstraint square8.order2.hdf -neumann -debug=63 >! junk

---- Dirichlet OK: 

tcmConstraint square8.order2.hdf -dirichlet -debug=63 >! junk

tcmConstraint square8.order2.hdf -dirichlet




****** July 1, 2015

mpirun -np 4 tcm3 square32.order2.hdf -solver=petsc -testCommunicator




mpirun -np 2 tcm3 square32.order2.hdf -solver=petsc

PETSC TEST ROUTINE: 
mpiexec -n 2  ex2

 mpiexec -n 2 ex2
Norm of error 0.000591671 iterations 118, m=101, n=101

****** June 30, 2015  --  TEST PETSc communicator

PETSC TEST ROUTINE: 
mpiexec -n 4  ex4

mpiexec -n 2 xterm -e gdb ./tcm3 
run square32.order2.hdf -solver=petsc -testCommunicator


mpirun -np 2 tcm3 square32.order2.hdf -solver=petsc
mpirun -np 1 tcm3 square32.order2.hdf -solver=petsc




*** 2015/05/16 -- more tests on variable coefficients

CIC: 
tcm3 cice2.order2 -mixed

cice4
grid=0 (square) max. rel. err=7.683513e-06 (7.683513e-06 with ghost)
grid=1 (Annulus) max. rel. err=4.251094e-05 (4.251094e-05 with ghost)

cice2:
 grid=0 (square) max. rel. err=4.763309e-05 (4.763309e-05 with ghost)
 grid=1 (Annulus) max. rel. err=2.225289e-04 (2.225289e-04 with ghost)


tcm3 square20pn.order2 -mixed

** square + MIXED BC: exact:
tcm3 square8.order2 -mixed
 grid=0 (square) max. rel. err=6.145234e-16 (9.217852e-16 with ghost)
yale/square8.order2///mixed: error                                : err = 6.15e-16, cpu=1.7e-05(s)  




tcm3 cice2.order2  

*** tcm3 square20.order2


.....solver: size = 2.38e+05 (bytes), grid-pts=625, reals/grid-pt=47.51 
 grid=0 (square) max. rel. err=1.255672e-15 (1.255672e-15 with ghost)
Maximum relative error with dirichlet bc's= 1.255672e-15 (1.255672e-15 with ghost)
yale/square20.order2///dirichlet: error                                                     : err = 1.26e-15, cpu=2.9e-05(s)  
Oges::allocateWorkSpace: numberOfNonzeros=3366 fillinRatio=2.000000e+01
allocateWorkSpace: numberOfEquations=625, nsp = 67320, fillinRatio= 20, numberOfNonzeros = 3366
residual=0.00e+00, time for 1st solve of the Neumann problem = 1.77e-03 (iterations=0)
residual=0.00e+00, time for 2nd solve of the Neumann problem = 3.10e-05 (iterations=0)
 grid=0 (square) max. rel. err=4.101861e-15 (4.185572e-15 with ghost)
Maximum relative error with neumann bc's= 4.101861e-15
yale/square20.order2///neumann: error                                                       : err = 4.10e-15, cpu=3.1e-05(s)  



*** 2014/05/18 -- test variable coefficient BC's

tbcc square16.order2 | grep mixed
square5/square/order=2/std/mixed (var-coeff)          : err = 5.33e-15, cpu=1.5e-04(s)  
cic/square/order=2/std/mixed (var-coeff)              : err = 1.24e-14, cpu=3.8e-04(s)  
cic/Annulus/order=2/std/mixed (var-coeff)             : err = 6.22e-15, cpu=1.8e-04(s)  
sib/box/order=2/std/mixed (var-coeff)                 : err = 1.42e-14, cpu=1.1e-02(s)  
sib/north-pole/order=2/std/mixed (var-coeff)          : err = 4.44e-15, cpu=9.1e-04(s)  
sib/south-pole/order=2/std/mixed (var-coeff)          : err = 4.22e-15, cpu=9.0e-04(s)  

checkop.p tbcc
Running: ./tbcc  > tbcc.out
 compare files tbcc.dp.check.new and tbcc.dp.check 
smartDiff: files tbcc.dp.check.new and tbcc.dp.check agree.
      ...tbcc appears to be correct
==================================================
============ Test apparently successful ==========
==================================================




tbc square16.order2



Regression test for tbc : 
checkop.p tbc 

running tbc...
Running: ./tbc  > tbc.out
 compare files tbc.dp.check.new and tbc.dp.check 
smartDiff: files tbc.dp.check.new and tbc.dp.check agree.
      ...tbc appears to be correct
==================================================
============ Test apparently successful ==========
==================================================